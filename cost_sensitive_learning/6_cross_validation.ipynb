{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9f40bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from lib.creditcard_fraud_dataset import get_train_test_dfs, X_COLS\n",
    "from lib.cs_eval import evaluate_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad03c0cf-465a-4f71-bdb1-5e83e24cb8e0",
   "metadata": {},
   "source": [
    "# 7. Cross-validation\n",
    "\n",
    "In the previous notebooks, we have seen multiple approaches to make a classifier cost-sensitive. Which one should we choose? To make an informed decision, we can employ **K-fold cross-validation**. The idea is to split up the training dataset into $K$ equally-sized partitions (*folds*) and to train each model $K$ times, each time with a different combination of $K - 1$ folds for training and $1$ fold for validation (evaluation). This is shown in the following figure:\n",
    "\n",
    "<img style=\"width: 50%; margin: auto\" src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\"/>\n",
    "\n",
    "The method that achieves the highest average performance can then be seen as the *best* one for the given data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb15fa77",
   "metadata": {},
   "source": [
    "## 6.1 Define function for each training method\n",
    "\n",
    "### 6.1.1 Baseline\n",
    "\n",
    "Logistic regression on the entire dataset, without any modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6678c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baseline(df_train):\n",
    "    X_train = df_train[X_COLS]\n",
    "    y_train = df_train['Class']\n",
    "\n",
    "    clf = LogisticRegression(max_iter=500)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179c9bce",
   "metadata": {},
   "source": [
    "### 6.1.2 Sample weighting\n",
    "\n",
    "Samples with a **higher misclassification cost contribute more** to the loss than samples with a lower misclassification cost.\n",
    "\n",
    "See [1_sample_weighting.ipynb](1_sample_weighting.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a33c1f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sample_weighted(df_train):\n",
    "    X_train = df_train[X_COLS]\n",
    "    y_train = df_train['Class']\n",
    "\n",
    "    clf_weighted = LogisticRegression(max_iter=500)\n",
    "\n",
    "    clf_weighted.fit(X_train, y_train, sample_weight=df_train['C_misclf'])\n",
    "\n",
    "    return clf_weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b726249",
   "metadata": {},
   "source": [
    "### 6.1.3 Subsampling\n",
    "\n",
    "See [2_subsampling.ipynb](2_subsampling.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "768cef49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_subsampled(df_train):\n",
    "    X_train = df_train[X_COLS]\n",
    "    y_train = df_train['Class']\n",
    "\n",
    "    is_fraud = y_train == 1\n",
    "    X_fraud = X_train[is_fraud]\n",
    "\n",
    "    num_fraud = y_train.sum()\n",
    "\n",
    "    no_fraud = y_train == 0\n",
    "    X_no_fraud = X_train[no_fraud]\n",
    "    X_no_fraud = X_no_fraud.sample(n=num_fraud)\n",
    "\n",
    "    X_balanced = pd.concat([X_fraud, X_no_fraud])\n",
    "    y_balanced = np.concatenate([np.ones(len(X_fraud)),\n",
    "                                 np.zeros(len(X_no_fraud))])\n",
    "\n",
    "    clf_balanced = LogisticRegression(max_iter=500)\n",
    "    clf_balanced.fit(X_balanced, y_balanced)\n",
    "    return clf_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afd9b32",
   "metadata": {},
   "source": [
    "### 6.1.4 Cost-sensitive sampling\n",
    "\n",
    "See [3_cost_sensitive_sampling.ipynb](3_cost_sensitive_sampling.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f347647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cost_sensitive_sampling_ratios(df_train):\n",
    "    n_pos = df_train['Class'].sum()\n",
    "    n_neg = len(df_train) - n_pos\n",
    "\n",
    "    return (df_train['C_FN'] / df_train['C_FP']) * (n_pos / n_neg)\n",
    "\n",
    "\n",
    "def train_cs_sampling(df_train):\n",
    "    X_train = df_train[X_COLS]\n",
    "    y_train = df_train['Class']\n",
    "\n",
    "    is_fraud = y_train == 1\n",
    "    X_fraud = X_train[is_fraud]\n",
    "\n",
    "    # Compute cost-sensitive positive-negative ratio\n",
    "    record_spec_r_cs = get_cost_sensitive_sampling_ratios(df_train)\n",
    "    global_r_cs = record_spec_r_cs.mean()\n",
    "\n",
    "    # Compute number of non-fraudulent transactions to sample\n",
    "    num_fraud = len(X_fraud)\n",
    "    num_no_fraud_sample = int(num_fraud / global_r_cs)\n",
    "\n",
    "    # Define X_no_fraud by sampling from all non-fraudulent rows\n",
    "    no_fraud = df_train['Class'] == 0\n",
    "    X_no_fraud = X_train[no_fraud]\n",
    "    X_no_fraud = X_no_fraud.sample(n=num_no_fraud_sample)\n",
    "\n",
    "    X_cs = pd.concat([X_fraud, X_no_fraud])\n",
    "    y_cs = np.concatenate([np.ones(len(X_fraud)),\n",
    "                           np.zeros(len(X_no_fraud))])\n",
    "\n",
    "    # Train the classifier\n",
    "    clf_cs = LogisticRegression(max_iter=500)\n",
    "    clf_cs.fit(X_cs, y_cs)\n",
    "\n",
    "    return clf_cs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81fa501",
   "metadata": {},
   "source": [
    "### 6.1.5 Cost-sensitive threshold\n",
    "\n",
    "See [4_cost_sensitive_threshold.ipynb](4_cost_sensitive_threshold.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f75ccd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_pred_cs_threshold(clf, df_val):\n",
    "    X_val = df_val[X_COLS]\n",
    "\n",
    "    # Get probability estimates\n",
    "    y_proba = clf.predict_proba(X_val)\n",
    "\n",
    "    # Classification with cost-sensitive threshold\n",
    "    cs_thresh = df_val['C_FP'] / (df_val['C_FP'] + df_val['C_FN'])\n",
    "    y_pred = y_proba[:, 1] > cs_thresh\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f55c885",
   "metadata": {},
   "source": [
    "### 6.1.6 AdaBoost with cost-sensitive weight initialization\n",
    "\n",
    "See [5_adaboost_weight_init.ipynb](5_adaboost_weight_init.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbad99a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unweighted_adaboost(df_train):\n",
    "    X_train = df_train[X_COLS]\n",
    "    y_train = df_train['Class']\n",
    "\n",
    "    clf_ada_unweighted = AdaBoostClassifier(n_estimators=10)\n",
    "    clf_ada_unweighted.fit(X_train, y_train)\n",
    "\n",
    "    return clf_ada_unweighted\n",
    "\n",
    "\n",
    "def train_weighted_adaboost(df_train):\n",
    "    X_train = df_train[X_COLS]\n",
    "    y_train = df_train['Class']\n",
    "\n",
    "    clf_ada_weighted = AdaBoostClassifier(n_estimators=10)\n",
    "    clf_ada_weighted.fit(X_train, y_train, sample_weight=df_train['C_misclf'])\n",
    "\n",
    "    return clf_ada_weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e5497f",
   "metadata": {},
   "source": [
    "## 6.2 Run cross-validation\n",
    "\n",
    "### 6.2.1 Create folds\n",
    "\n",
    "We will run a 5-fold cross-validation. To create the folds, we can use [`StratifiedKFold` of SciKit-Learn](https://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold). With `StratifiedKFold`, each split is guaranteed to contain a comparable distribution of fraudulent and non-fraudulent transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a23cfc4d-d683-4efa-a944-cad6cde6e34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855b7598-6a8f-4991-bcf3-b7a30aebcee2",
   "metadata": {},
   "source": [
    "On a `StratifiedKFold` object, you can call [the method `split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold.split) to generate the splits for a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffa25fa6-fdc9-4ac3-a15f-c579da091e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trainval, df_test = get_train_test_dfs()\n",
    "\n",
    "X_trainval = df_trainval[X_COLS]\n",
    "y_trainval = df_trainval['Class']\n",
    "\n",
    "split_idxs = skf.split(X_trainval, y_trainval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdca45d-05f9-477d-ae5b-c943db097d3e",
   "metadata": {},
   "source": [
    "### 6.2.2  Loop over splits\n",
    "\n",
    "In each iteration, we use $4$ folds for training with each of the above training methods, and the other fold for validation. For each validation result, we append a new Python dictionary to the list `results`.\n",
    "\n",
    "Note that `split()` returns the *indices* of the splits, not the samples themselves. In the `for`-loop below, you can indeed see that we use the `train_index` and `val_index` to select rows from `df_trainval`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb302b7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Baseline...\n",
      "1. Sample weighting...\n",
      "2. Subsampling...\n",
      "3. Cost-sensitive sampling\n",
      "4. Cost-sensitive threhold\n",
      "5a. AdaBoost, unweighted...\n",
      "5b. AdaBoost, weighted...\n",
      "\n",
      "0. Baseline...\n",
      "1. Sample weighting...\n",
      "2. Subsampling...\n",
      "3. Cost-sensitive sampling\n",
      "4. Cost-sensitive threhold\n",
      "5a. AdaBoost, unweighted...\n",
      "5b. AdaBoost, weighted...\n",
      "\n",
      "0. Baseline...\n",
      "1. Sample weighting...\n",
      "2. Subsampling...\n",
      "3. Cost-sensitive sampling\n",
      "4. Cost-sensitive threhold\n",
      "5a. AdaBoost, unweighted...\n",
      "5b. AdaBoost, weighted...\n",
      "\n",
      "0. Baseline...\n",
      "1. Sample weighting...\n",
      "2. Subsampling...\n",
      "3. Cost-sensitive sampling\n",
      "4. Cost-sensitive threhold\n",
      "5a. AdaBoost, unweighted...\n",
      "5b. AdaBoost, weighted...\n",
      "\n",
      "0. Baseline...\n",
      "1. Sample weighting...\n",
      "2. Subsampling...\n",
      "3. Cost-sensitive sampling\n",
      "4. Cost-sensitive threhold\n",
      "5a. AdaBoost, unweighted...\n",
      "5b. AdaBoost, weighted...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for train_index, val_index in split_idxs:\n",
    "    # Create train and val set from given indices\n",
    "    df_train = df_trainval.iloc[train_index]\n",
    "    df_val = df_trainval.iloc[val_index]\n",
    "\n",
    "    X_val = df_val[X_COLS]\n",
    "    y_val = df_val['Class']\n",
    "    amounts = df_val['Amount']\n",
    "\n",
    "    # 0. Logistic regression on unmodified dataset\n",
    "    print('0. Baseline...')\n",
    "    clf_baseline = train_baseline(df_train)\n",
    "    y_pred_baseline = clf_baseline.predict(X_val)\n",
    "    eval_baseline = evaluate_pred(y_val, y_pred_baseline, amounts)\n",
    "    results.append({\n",
    "        'Method': 'Baseline',\n",
    "        **eval_baseline\n",
    "    })\n",
    "\n",
    "    # 1. Sample weighting\n",
    "    print('1. Sample weighting...')\n",
    "    clf_weighted = train_sample_weighted(df_train)\n",
    "    y_pred_weighted = clf_weighted.predict(X_val)\n",
    "    eval_weighted = evaluate_pred(y_val, y_pred_weighted, amounts)\n",
    "    results.append({\n",
    "        'Method': 'Sample weighting',\n",
    "        **eval_weighted\n",
    "    })\n",
    "\n",
    "    # 2. Subsampling\n",
    "    print('2. Subsampling...')\n",
    "    clf_subsamp = train_subsampled(df_train)\n",
    "    y_pred_subsamp = clf_subsamp.predict(X_val)\n",
    "    eval_subsamp = evaluate_pred(y_val, y_pred_subsamp, amounts)\n",
    "    results.append({\n",
    "        'Method': 'Subsampling',\n",
    "        **eval_subsamp\n",
    "    })\n",
    "\n",
    "    # 3. Cost-sensitive sampling\n",
    "    print('3. Cost-sensitive sampling')\n",
    "    clf_cs_samp = train_cs_sampling(df_train)\n",
    "    y_pred_cs_samp = clf_cs_samp.predict(X_val)\n",
    "    eval_cs_samp = evaluate_pred(y_val, y_pred_cs_samp, amounts)\n",
    "    results.append({\n",
    "        'Method': 'Cost-sensitive sampling',\n",
    "        **eval_cs_samp\n",
    "    })\n",
    "\n",
    "    # 4. Cost-sensitive threhold\n",
    "    print('4. Cost-sensitive threhold')\n",
    "    y_pred_cs_threshold = get_y_pred_cs_threshold(clf_baseline, df_val)\n",
    "    eval_cs_threshold = evaluate_pred(y_val, y_pred_cs_threshold, amounts)\n",
    "    results.append({\n",
    "        'Method': 'Cost-sensitive threhold',\n",
    "        **eval_cs_threshold\n",
    "    })\n",
    "\n",
    "    # 5a. AdaBoost, unweighted\n",
    "    print('5a. AdaBoost, unweighted...')\n",
    "    clf_unwght_ada = train_unweighted_adaboost(df_train)\n",
    "    y_pred_unwght_ada = clf_unwght_ada.predict(X_val)\n",
    "    eval_unwght_ada = evaluate_pred(y_val, y_pred_unwght_ada, amounts)\n",
    "    results.append({\n",
    "        'Method': 'AdaBoost (unweighted)',\n",
    "        **eval_unwght_ada\n",
    "    })\n",
    "\n",
    "    # 5b. AdaBoost, weighted\n",
    "    print('5b. AdaBoost, weighted...')\n",
    "    clf_wght_ada = train_weighted_adaboost(df_train)\n",
    "    y_pred_wght_ada = clf_wght_ada.predict(X_val)\n",
    "    eval_wght_ada = evaluate_pred(y_val, y_pred_wght_ada, amounts)\n",
    "    results.append({\n",
    "        'Method': 'AdaBoost (weighted)',\n",
    "        **eval_wght_ada\n",
    "    })\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2c4219-35c8-4df0-8c2e-c6bdbdd35c28",
   "metadata": {},
   "source": [
    "## 6.3 Inspect the results\n",
    "\n",
    "We start by converting the list of dictionaries `results` into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bd3939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539b5961-3052-4376-ac24-a6f7dec9dc3b",
   "metadata": {},
   "source": [
    "To obtain a single metric, we can compute the *F1-score*. This is the harmonic mean between the precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c90acec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Cost Precision</th>\n",
       "      <th>Cost Recall</th>\n",
       "      <th>TP Amount</th>\n",
       "      <th>FP Amount</th>\n",
       "      <th>FN Amount</th>\n",
       "      <th>Net Recovered Amount</th>\n",
       "      <th>Cost F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.996606</td>\n",
       "      <td>0.535089</td>\n",
       "      <td>2935.97</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2550.91</td>\n",
       "      <td>375.06</td>\n",
       "      <td>0.696317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sample weighting</td>\n",
       "      <td>0.953153</td>\n",
       "      <td>0.734204</td>\n",
       "      <td>4028.49</td>\n",
       "      <td>198.0</td>\n",
       "      <td>1458.39</td>\n",
       "      <td>2372.10</td>\n",
       "      <td>0.829473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subsampling</td>\n",
       "      <td>0.738263</td>\n",
       "      <td>0.880083</td>\n",
       "      <td>4828.91</td>\n",
       "      <td>1712.0</td>\n",
       "      <td>657.97</td>\n",
       "      <td>2458.94</td>\n",
       "      <td>0.802959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cost-sensitive sampling</td>\n",
       "      <td>0.978147</td>\n",
       "      <td>0.734204</td>\n",
       "      <td>4028.49</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1458.39</td>\n",
       "      <td>2480.10</td>\n",
       "      <td>0.838800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cost-sensitive threhold</td>\n",
       "      <td>0.961160</td>\n",
       "      <td>0.721620</td>\n",
       "      <td>3959.44</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1527.44</td>\n",
       "      <td>2272.00</td>\n",
       "      <td>0.824341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AdaBoost (unweighted)</td>\n",
       "      <td>0.995070</td>\n",
       "      <td>0.441464</td>\n",
       "      <td>2422.26</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3064.62</td>\n",
       "      <td>-654.36</td>\n",
       "      <td>0.611594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AdaBoost (weighted)</td>\n",
       "      <td>0.948552</td>\n",
       "      <td>0.732529</td>\n",
       "      <td>4019.30</td>\n",
       "      <td>218.0</td>\n",
       "      <td>1467.58</td>\n",
       "      <td>2333.72</td>\n",
       "      <td>0.826661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.997314</td>\n",
       "      <td>0.186850</td>\n",
       "      <td>1485.30</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6463.87</td>\n",
       "      <td>-4982.57</td>\n",
       "      <td>0.314733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sample weighting</td>\n",
       "      <td>0.966376</td>\n",
       "      <td>0.838821</td>\n",
       "      <td>6667.93</td>\n",
       "      <td>232.0</td>\n",
       "      <td>1281.24</td>\n",
       "      <td>5154.69</td>\n",
       "      <td>0.898092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Subsampling</td>\n",
       "      <td>0.714760</td>\n",
       "      <td>0.645593</td>\n",
       "      <td>5131.93</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>2817.24</td>\n",
       "      <td>266.69</td>\n",
       "      <td>0.678418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Cost-sensitive sampling</td>\n",
       "      <td>0.965562</td>\n",
       "      <td>0.571388</td>\n",
       "      <td>4542.06</td>\n",
       "      <td>162.0</td>\n",
       "      <td>3407.11</td>\n",
       "      <td>972.95</td>\n",
       "      <td>0.717929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Cost-sensitive threhold</td>\n",
       "      <td>0.962187</td>\n",
       "      <td>0.569789</td>\n",
       "      <td>4529.35</td>\n",
       "      <td>178.0</td>\n",
       "      <td>3419.82</td>\n",
       "      <td>931.53</td>\n",
       "      <td>0.715734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AdaBoost (unweighted)</td>\n",
       "      <td>0.997138</td>\n",
       "      <td>0.525880</td>\n",
       "      <td>4180.31</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3768.86</td>\n",
       "      <td>399.45</td>\n",
       "      <td>0.688600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AdaBoost (weighted)</td>\n",
       "      <td>0.971952</td>\n",
       "      <td>0.619017</td>\n",
       "      <td>4920.67</td>\n",
       "      <td>142.0</td>\n",
       "      <td>3028.50</td>\n",
       "      <td>1750.17</td>\n",
       "      <td>0.756337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.996685</td>\n",
       "      <td>0.529965</td>\n",
       "      <td>2405.28</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2133.28</td>\n",
       "      <td>264.00</td>\n",
       "      <td>0.691984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Sample weighting</td>\n",
       "      <td>0.940431</td>\n",
       "      <td>0.765269</td>\n",
       "      <td>3473.22</td>\n",
       "      <td>220.0</td>\n",
       "      <td>1065.34</td>\n",
       "      <td>2187.88</td>\n",
       "      <td>0.843856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Subsampling</td>\n",
       "      <td>0.660595</td>\n",
       "      <td>0.990629</td>\n",
       "      <td>4496.03</td>\n",
       "      <td>2310.0</td>\n",
       "      <td>42.53</td>\n",
       "      <td>2143.50</td>\n",
       "      <td>0.792630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Cost-sensitive sampling</td>\n",
       "      <td>0.941451</td>\n",
       "      <td>0.765269</td>\n",
       "      <td>3473.22</td>\n",
       "      <td>216.0</td>\n",
       "      <td>1065.34</td>\n",
       "      <td>2191.88</td>\n",
       "      <td>0.844267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Cost-sensitive threhold</td>\n",
       "      <td>0.953531</td>\n",
       "      <td>0.696260</td>\n",
       "      <td>3160.02</td>\n",
       "      <td>154.0</td>\n",
       "      <td>1378.54</td>\n",
       "      <td>1627.48</td>\n",
       "      <td>0.804836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>AdaBoost (unweighted)</td>\n",
       "      <td>0.996279</td>\n",
       "      <td>0.707962</td>\n",
       "      <td>3213.13</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1325.43</td>\n",
       "      <td>1875.70</td>\n",
       "      <td>0.827733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>AdaBoost (weighted)</td>\n",
       "      <td>0.969438</td>\n",
       "      <td>0.880625</td>\n",
       "      <td>3996.77</td>\n",
       "      <td>126.0</td>\n",
       "      <td>541.79</td>\n",
       "      <td>3328.98</td>\n",
       "      <td>0.922900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.993544</td>\n",
       "      <td>0.558022</td>\n",
       "      <td>2770.27</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2194.18</td>\n",
       "      <td>558.09</td>\n",
       "      <td>0.714658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Sample weighting</td>\n",
       "      <td>0.932975</td>\n",
       "      <td>0.829957</td>\n",
       "      <td>4120.28</td>\n",
       "      <td>296.0</td>\n",
       "      <td>844.17</td>\n",
       "      <td>2980.11</td>\n",
       "      <td>0.878456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Subsampling</td>\n",
       "      <td>0.674301</td>\n",
       "      <td>0.830720</td>\n",
       "      <td>4124.07</td>\n",
       "      <td>1992.0</td>\n",
       "      <td>840.38</td>\n",
       "      <td>1291.69</td>\n",
       "      <td>0.744382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Cost-sensitive sampling</td>\n",
       "      <td>0.953265</td>\n",
       "      <td>0.829957</td>\n",
       "      <td>4120.28</td>\n",
       "      <td>202.0</td>\n",
       "      <td>844.17</td>\n",
       "      <td>3074.11</td>\n",
       "      <td>0.887348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Cost-sensitive threhold</td>\n",
       "      <td>0.959015</td>\n",
       "      <td>0.829554</td>\n",
       "      <td>4118.28</td>\n",
       "      <td>176.0</td>\n",
       "      <td>846.17</td>\n",
       "      <td>3096.11</td>\n",
       "      <td>0.889599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>AdaBoost (unweighted)</td>\n",
       "      <td>0.992594</td>\n",
       "      <td>0.701907</td>\n",
       "      <td>3484.58</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1479.87</td>\n",
       "      <td>1978.71</td>\n",
       "      <td>0.822317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>AdaBoost (weighted)</td>\n",
       "      <td>0.941859</td>\n",
       "      <td>0.763567</td>\n",
       "      <td>3790.69</td>\n",
       "      <td>234.0</td>\n",
       "      <td>1173.76</td>\n",
       "      <td>2382.93</td>\n",
       "      <td>0.843393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.997943</td>\n",
       "      <td>0.490781</td>\n",
       "      <td>2911.48</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3020.86</td>\n",
       "      <td>-115.38</td>\n",
       "      <td>0.657975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Sample weighting</td>\n",
       "      <td>0.956266</td>\n",
       "      <td>0.796134</td>\n",
       "      <td>4722.94</td>\n",
       "      <td>216.0</td>\n",
       "      <td>1209.40</td>\n",
       "      <td>3297.54</td>\n",
       "      <td>0.868884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Subsampling</td>\n",
       "      <td>0.748083</td>\n",
       "      <td>0.999142</td>\n",
       "      <td>5927.25</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>5.09</td>\n",
       "      <td>3926.16</td>\n",
       "      <td>0.855575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Cost-sensitive sampling</td>\n",
       "      <td>0.972817</td>\n",
       "      <td>0.796303</td>\n",
       "      <td>4723.94</td>\n",
       "      <td>132.0</td>\n",
       "      <td>1208.40</td>\n",
       "      <td>3383.54</td>\n",
       "      <td>0.875754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Cost-sensitive threhold</td>\n",
       "      <td>0.973229</td>\n",
       "      <td>0.992753</td>\n",
       "      <td>5889.35</td>\n",
       "      <td>162.0</td>\n",
       "      <td>42.99</td>\n",
       "      <td>5684.36</td>\n",
       "      <td>0.982894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>AdaBoost (unweighted)</td>\n",
       "      <td>0.996572</td>\n",
       "      <td>0.686127</td>\n",
       "      <td>4070.34</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1862.00</td>\n",
       "      <td>2194.34</td>\n",
       "      <td>0.812712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>AdaBoost (weighted)</td>\n",
       "      <td>0.960914</td>\n",
       "      <td>0.787391</td>\n",
       "      <td>4671.07</td>\n",
       "      <td>190.0</td>\n",
       "      <td>1261.27</td>\n",
       "      <td>3219.80</td>\n",
       "      <td>0.865541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Method  Cost Precision  Cost Recall  TP Amount  \\\n",
       "0                  Baseline        0.996606     0.535089    2935.97   \n",
       "1          Sample weighting        0.953153     0.734204    4028.49   \n",
       "2               Subsampling        0.738263     0.880083    4828.91   \n",
       "3   Cost-sensitive sampling        0.978147     0.734204    4028.49   \n",
       "4   Cost-sensitive threhold        0.961160     0.721620    3959.44   \n",
       "5     AdaBoost (unweighted)        0.995070     0.441464    2422.26   \n",
       "6       AdaBoost (weighted)        0.948552     0.732529    4019.30   \n",
       "7                  Baseline        0.997314     0.186850    1485.30   \n",
       "8          Sample weighting        0.966376     0.838821    6667.93   \n",
       "9               Subsampling        0.714760     0.645593    5131.93   \n",
       "10  Cost-sensitive sampling        0.965562     0.571388    4542.06   \n",
       "11  Cost-sensitive threhold        0.962187     0.569789    4529.35   \n",
       "12    AdaBoost (unweighted)        0.997138     0.525880    4180.31   \n",
       "13      AdaBoost (weighted)        0.971952     0.619017    4920.67   \n",
       "14                 Baseline        0.996685     0.529965    2405.28   \n",
       "15         Sample weighting        0.940431     0.765269    3473.22   \n",
       "16              Subsampling        0.660595     0.990629    4496.03   \n",
       "17  Cost-sensitive sampling        0.941451     0.765269    3473.22   \n",
       "18  Cost-sensitive threhold        0.953531     0.696260    3160.02   \n",
       "19    AdaBoost (unweighted)        0.996279     0.707962    3213.13   \n",
       "20      AdaBoost (weighted)        0.969438     0.880625    3996.77   \n",
       "21                 Baseline        0.993544     0.558022    2770.27   \n",
       "22         Sample weighting        0.932975     0.829957    4120.28   \n",
       "23              Subsampling        0.674301     0.830720    4124.07   \n",
       "24  Cost-sensitive sampling        0.953265     0.829957    4120.28   \n",
       "25  Cost-sensitive threhold        0.959015     0.829554    4118.28   \n",
       "26    AdaBoost (unweighted)        0.992594     0.701907    3484.58   \n",
       "27      AdaBoost (weighted)        0.941859     0.763567    3790.69   \n",
       "28                 Baseline        0.997943     0.490781    2911.48   \n",
       "29         Sample weighting        0.956266     0.796134    4722.94   \n",
       "30              Subsampling        0.748083     0.999142    5927.25   \n",
       "31  Cost-sensitive sampling        0.972817     0.796303    4723.94   \n",
       "32  Cost-sensitive threhold        0.973229     0.992753    5889.35   \n",
       "33    AdaBoost (unweighted)        0.996572     0.686127    4070.34   \n",
       "34      AdaBoost (weighted)        0.960914     0.787391    4671.07   \n",
       "\n",
       "    FP Amount  FN Amount  Net Recovered Amount   Cost F1  \n",
       "0        10.0    2550.91                375.06  0.696317  \n",
       "1       198.0    1458.39               2372.10  0.829473  \n",
       "2      1712.0     657.97               2458.94  0.802959  \n",
       "3        90.0    1458.39               2480.10  0.838800  \n",
       "4       160.0    1527.44               2272.00  0.824341  \n",
       "5        12.0    3064.62               -654.36  0.611594  \n",
       "6       218.0    1467.58               2333.72  0.826661  \n",
       "7         4.0    6463.87              -4982.57  0.314733  \n",
       "8       232.0    1281.24               5154.69  0.898092  \n",
       "9      2048.0    2817.24                266.69  0.678418  \n",
       "10      162.0    3407.11                972.95  0.717929  \n",
       "11      178.0    3419.82                931.53  0.715734  \n",
       "12       12.0    3768.86                399.45  0.688600  \n",
       "13      142.0    3028.50               1750.17  0.756337  \n",
       "14        8.0    2133.28                264.00  0.691984  \n",
       "15      220.0    1065.34               2187.88  0.843856  \n",
       "16     2310.0      42.53               2143.50  0.792630  \n",
       "17      216.0    1065.34               2191.88  0.844267  \n",
       "18      154.0    1378.54               1627.48  0.804836  \n",
       "19       12.0    1325.43               1875.70  0.827733  \n",
       "20      126.0     541.79               3328.98  0.922900  \n",
       "21       18.0    2194.18                558.09  0.714658  \n",
       "22      296.0     844.17               2980.11  0.878456  \n",
       "23     1992.0     840.38               1291.69  0.744382  \n",
       "24      202.0     844.17               3074.11  0.887348  \n",
       "25      176.0     846.17               3096.11  0.889599  \n",
       "26       26.0    1479.87               1978.71  0.822317  \n",
       "27      234.0    1173.76               2382.93  0.843393  \n",
       "28        6.0    3020.86               -115.38  0.657975  \n",
       "29      216.0    1209.40               3297.54  0.868884  \n",
       "30     1996.0       5.09               3926.16  0.855575  \n",
       "31      132.0    1208.40               3383.54  0.875754  \n",
       "32      162.0      42.99               5684.36  0.982894  \n",
       "33       14.0    1862.00               2194.34  0.812712  \n",
       "34      190.0    1261.27               3219.80  0.865541  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results['Cost F1'] = (\n",
    "    2 * df_results['Cost Precision'] * df_results['Cost Recall']\n",
    "    / (df_results['Cost Precision'] + df_results['Cost Recall'])\n",
    ")\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9ff9c8-754e-4589-a271-ba27d7c7e5a5",
   "metadata": {},
   "source": [
    "To obtain a single F1-score per training method, we can [group by](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) the `'Method'` column and compute the mean per group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0ff4576-734c-4bda-a014-ddf41fd5fa08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cost Precision</th>\n",
       "      <th>Cost Recall</th>\n",
       "      <th>TP Amount</th>\n",
       "      <th>FP Amount</th>\n",
       "      <th>FN Amount</th>\n",
       "      <th>Net Recovered Amount</th>\n",
       "      <th>Cost F1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AdaBoost (unweighted)</th>\n",
       "      <td>0.995531</td>\n",
       "      <td>0.612668</td>\n",
       "      <td>3474.124</td>\n",
       "      <td>15.2</td>\n",
       "      <td>2300.156</td>\n",
       "      <td>1158.768</td>\n",
       "      <td>0.752591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost (weighted)</th>\n",
       "      <td>0.958543</td>\n",
       "      <td>0.756626</td>\n",
       "      <td>4279.700</td>\n",
       "      <td>182.0</td>\n",
       "      <td>1494.580</td>\n",
       "      <td>2603.120</td>\n",
       "      <td>0.842966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.996419</td>\n",
       "      <td>0.460141</td>\n",
       "      <td>2501.660</td>\n",
       "      <td>9.2</td>\n",
       "      <td>3272.620</td>\n",
       "      <td>-780.160</td>\n",
       "      <td>0.615133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cost-sensitive sampling</th>\n",
       "      <td>0.962248</td>\n",
       "      <td>0.739424</td>\n",
       "      <td>4177.598</td>\n",
       "      <td>160.4</td>\n",
       "      <td>1596.682</td>\n",
       "      <td>2420.516</td>\n",
       "      <td>0.832819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cost-sensitive threhold</th>\n",
       "      <td>0.961824</td>\n",
       "      <td>0.761995</td>\n",
       "      <td>4331.288</td>\n",
       "      <td>166.0</td>\n",
       "      <td>1442.992</td>\n",
       "      <td>2722.296</td>\n",
       "      <td>0.843481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample weighting</th>\n",
       "      <td>0.949840</td>\n",
       "      <td>0.792877</td>\n",
       "      <td>4602.572</td>\n",
       "      <td>232.4</td>\n",
       "      <td>1171.708</td>\n",
       "      <td>3198.464</td>\n",
       "      <td>0.863752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subsampling</th>\n",
       "      <td>0.707200</td>\n",
       "      <td>0.869234</td>\n",
       "      <td>4901.638</td>\n",
       "      <td>2011.6</td>\n",
       "      <td>872.642</td>\n",
       "      <td>2017.396</td>\n",
       "      <td>0.774793</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Cost Precision  Cost Recall  TP Amount  FP Amount  \\\n",
       "Method                                                                       \n",
       "AdaBoost (unweighted)          0.995531     0.612668   3474.124       15.2   \n",
       "AdaBoost (weighted)            0.958543     0.756626   4279.700      182.0   \n",
       "Baseline                       0.996419     0.460141   2501.660        9.2   \n",
       "Cost-sensitive sampling        0.962248     0.739424   4177.598      160.4   \n",
       "Cost-sensitive threhold        0.961824     0.761995   4331.288      166.0   \n",
       "Sample weighting               0.949840     0.792877   4602.572      232.4   \n",
       "Subsampling                    0.707200     0.869234   4901.638     2011.6   \n",
       "\n",
       "                         FN Amount  Net Recovered Amount   Cost F1  \n",
       "Method                                                              \n",
       "AdaBoost (unweighted)     2300.156              1158.768  0.752591  \n",
       "AdaBoost (weighted)       1494.580              2603.120  0.842966  \n",
       "Baseline                  3272.620              -780.160  0.615133  \n",
       "Cost-sensitive sampling   1596.682              2420.516  0.832819  \n",
       "Cost-sensitive threhold   1442.992              2722.296  0.843481  \n",
       "Sample weighting          1171.708              3198.464  0.863752  \n",
       "Subsampling                872.642              2017.396  0.774793  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_agg = df_results.groupby('Method').mean()\n",
    "df_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b288343-e411-45fa-879c-62a42b55cac6",
   "metadata": {},
   "source": [
    "Finally, we can sort the aggregated DataFrame by the `'Cost F1'` column to easily see the methods that work best on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c31a2151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cost Precision</th>\n",
       "      <th>Cost Recall</th>\n",
       "      <th>TP Amount</th>\n",
       "      <th>FP Amount</th>\n",
       "      <th>FN Amount</th>\n",
       "      <th>Net Recovered Amount</th>\n",
       "      <th>Cost F1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sample weighting</th>\n",
       "      <td>0.949840</td>\n",
       "      <td>0.792877</td>\n",
       "      <td>4602.572</td>\n",
       "      <td>232.4</td>\n",
       "      <td>1171.708</td>\n",
       "      <td>3198.464</td>\n",
       "      <td>0.863752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cost-sensitive threhold</th>\n",
       "      <td>0.961824</td>\n",
       "      <td>0.761995</td>\n",
       "      <td>4331.288</td>\n",
       "      <td>166.0</td>\n",
       "      <td>1442.992</td>\n",
       "      <td>2722.296</td>\n",
       "      <td>0.843481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost (weighted)</th>\n",
       "      <td>0.958543</td>\n",
       "      <td>0.756626</td>\n",
       "      <td>4279.700</td>\n",
       "      <td>182.0</td>\n",
       "      <td>1494.580</td>\n",
       "      <td>2603.120</td>\n",
       "      <td>0.842966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cost-sensitive sampling</th>\n",
       "      <td>0.962248</td>\n",
       "      <td>0.739424</td>\n",
       "      <td>4177.598</td>\n",
       "      <td>160.4</td>\n",
       "      <td>1596.682</td>\n",
       "      <td>2420.516</td>\n",
       "      <td>0.832819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subsampling</th>\n",
       "      <td>0.707200</td>\n",
       "      <td>0.869234</td>\n",
       "      <td>4901.638</td>\n",
       "      <td>2011.6</td>\n",
       "      <td>872.642</td>\n",
       "      <td>2017.396</td>\n",
       "      <td>0.774793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoost (unweighted)</th>\n",
       "      <td>0.995531</td>\n",
       "      <td>0.612668</td>\n",
       "      <td>3474.124</td>\n",
       "      <td>15.2</td>\n",
       "      <td>2300.156</td>\n",
       "      <td>1158.768</td>\n",
       "      <td>0.752591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <td>0.996419</td>\n",
       "      <td>0.460141</td>\n",
       "      <td>2501.660</td>\n",
       "      <td>9.2</td>\n",
       "      <td>3272.620</td>\n",
       "      <td>-780.160</td>\n",
       "      <td>0.615133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Cost Precision  Cost Recall  TP Amount  FP Amount  \\\n",
       "Method                                                                       \n",
       "Sample weighting               0.949840     0.792877   4602.572      232.4   \n",
       "Cost-sensitive threhold        0.961824     0.761995   4331.288      166.0   \n",
       "AdaBoost (weighted)            0.958543     0.756626   4279.700      182.0   \n",
       "Cost-sensitive sampling        0.962248     0.739424   4177.598      160.4   \n",
       "Subsampling                    0.707200     0.869234   4901.638     2011.6   \n",
       "AdaBoost (unweighted)          0.995531     0.612668   3474.124       15.2   \n",
       "Baseline                       0.996419     0.460141   2501.660        9.2   \n",
       "\n",
       "                         FN Amount  Net Recovered Amount   Cost F1  \n",
       "Method                                                              \n",
       "Sample weighting          1171.708              3198.464  0.863752  \n",
       "Cost-sensitive threhold   1442.992              2722.296  0.843481  \n",
       "AdaBoost (weighted)       1494.580              2603.120  0.842966  \n",
       "Cost-sensitive sampling   1596.682              2420.516  0.832819  \n",
       "Subsampling                872.642              2017.396  0.774793  \n",
       "AdaBoost (unweighted)     2300.156              1158.768  0.752591  \n",
       "Baseline                  3272.620              -780.160  0.615133  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_agg.sort_values(by='Cost F1', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3902e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
