{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f40bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from lib.creditcard_fraud_dataset import get_train_test_dfs, X_COLS\n",
    "from lib.cs_eval import evaluate_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad03c0cf-465a-4f71-bdb1-5e83e24cb8e0",
   "metadata": {},
   "source": [
    "# 7. Cross-validation\n",
    "\n",
    "In the previous notebooks, we have seen multiple approaches to make a classifier cost-sensitive. Which one should we choose? To make an informed decision, we can employ **K-fold cross-validation**. The idea is to split up the training dataset into $K$ equally-sized partitions (*folds*) and to train each model $K$ times, each time with a different combination of $K - 1$ folds for training and $1$ fold for validation (evaluation). This is shown in the following figure:\n",
    "\n",
    "<img style=\"width: 50%; margin: auto\" src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\"/>\n",
    "\n",
    "The method that achieves the highest average performance can then be seen as the *best* one for the given data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb15fa77",
   "metadata": {},
   "source": [
    "## 6.1 Define function for each training method\n",
    "\n",
    "### 6.1.1 Baseline\n",
    "\n",
    "Logistic regression on the entire dataset, without any modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6678c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baseline(df_train):\n",
    "    X_train = df_train[X_COLS]\n",
    "    y_train = df_train['Class']\n",
    "\n",
    "    clf = LogisticRegression(max_iter=500)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179c9bce",
   "metadata": {},
   "source": [
    "### 6.1.2 Sample weighting\n",
    "\n",
    "Samples with a **higher misclassification cost contribute more** to the loss than samples with a lower misclassification cost.\n",
    "\n",
    "See [1_sample_weighting.ipynb](1_sample_weighting.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33c1f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sample_weighted(df_train):\n",
    "    X_train = df_train[X_COLS]\n",
    "    y_train = df_train['Class']\n",
    "\n",
    "    clf_weighted = LogisticRegression(max_iter=500)\n",
    "\n",
    "    clf_weighted.fit(X_train, y_train, sample_weight=df_train['C_misclf'])\n",
    "\n",
    "    return clf_weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b726249",
   "metadata": {},
   "source": [
    "### 6.1.3 Subsampling\n",
    "\n",
    "See [2_subsampling.ipynb](2_subsampling.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768cef49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_subsampled(df_train):\n",
    "    X_train = df_train[X_COLS]\n",
    "    y_train = df_train['Class']\n",
    "\n",
    "    is_fraud = y_train == 1\n",
    "    X_fraud = X_train[is_fraud]\n",
    "\n",
    "    num_fraud = y_train.sum()\n",
    "\n",
    "    no_fraud = y_train == 0\n",
    "    X_no_fraud = X_train[no_fraud]\n",
    "    X_no_fraud = X_no_fraud.sample(n=num_fraud)\n",
    "\n",
    "    X_balanced = pd.concat([X_fraud, X_no_fraud])\n",
    "    y_balanced = np.concatenate([np.ones(len(X_fraud)),\n",
    "                                 np.zeros(len(X_no_fraud))])\n",
    "\n",
    "    clf_balanced = LogisticRegression(max_iter=500)\n",
    "    clf_balanced.fit(X_balanced, y_balanced)\n",
    "    return clf_balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afd9b32",
   "metadata": {},
   "source": [
    "### 6.1.4 Cost-sensitive sampling\n",
    "\n",
    "See [3_cost_sensitive_sampling.ipynb](3_cost_sensitive_sampling.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f347647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cost_sensitive_sampling_ratios(df_train):\n",
    "    n_pos = df_train['Class'].sum()\n",
    "    n_neg = len(df_train) - n_pos\n",
    "\n",
    "    return (df_train['C_FN'] / df_train['C_FP']) * (n_pos / n_neg)\n",
    "\n",
    "\n",
    "def train_cs_sampling(df_train):\n",
    "    X_train = df_train[X_COLS]\n",
    "    y_train = df_train['Class']\n",
    "\n",
    "    is_fraud = y_train == 1\n",
    "    X_fraud = X_train[is_fraud]\n",
    "\n",
    "    # Compute cost-sensitive positive-negative ratio\n",
    "    record_spec_r_cs = get_cost_sensitive_sampling_ratios(df_train)\n",
    "    global_r_cs = record_spec_r_cs.mean()\n",
    "\n",
    "    # Compute number of non-fraudulent transactions to sample\n",
    "    num_fraud = len(X_fraud)\n",
    "    num_no_fraud_sample = int(num_fraud / global_r_cs)\n",
    "\n",
    "    # Define X_no_fraud by sampling from all non-fraudulent rows\n",
    "    no_fraud = df_train['Class'] == 0\n",
    "    X_no_fraud = X_train[no_fraud]\n",
    "    X_no_fraud = X_no_fraud.sample(n=num_no_fraud_sample)\n",
    "\n",
    "    X_cs = pd.concat([X_fraud, X_no_fraud])\n",
    "    y_cs = np.concatenate([np.ones(len(X_fraud)),\n",
    "                           np.zeros(len(X_no_fraud))])\n",
    "\n",
    "    # Train the classifier\n",
    "    clf_cs = LogisticRegression(max_iter=500)\n",
    "    clf_cs.fit(X_cs, y_cs)\n",
    "\n",
    "    return clf_cs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81fa501",
   "metadata": {},
   "source": [
    "### 6.1.5 Cost-sensitive threshold\n",
    "\n",
    "See [4_cost_sensitive_threshold.ipynb](4_cost_sensitive_threshold.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75ccd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_pred_cs_threshold(clf, df_val):\n",
    "    X_val = df_val[X_COLS]\n",
    "\n",
    "    # Get probability estimates\n",
    "    y_proba = clf.predict_proba(X_val)\n",
    "\n",
    "    # Classification with cost-sensitive threshold\n",
    "    cs_thresh = df_val['C_FP'] / (df_val['C_FP'] + df_val['C_FN'])\n",
    "    y_pred = y_proba[:, 1] > cs_thresh\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f55c885",
   "metadata": {},
   "source": [
    "### 6.1.6 AdaBoost with cost-sensitive weight initialization\n",
    "\n",
    "See [5_adaboost_weight_init.ipynb](5_adaboost_weight_init.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbad99a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unweighted_adaboost(df_train):\n",
    "    X_train = df_train[X_COLS]\n",
    "    y_train = df_train['Class']\n",
    "\n",
    "    clf_ada_unweighted = AdaBoostClassifier(n_estimators=10)\n",
    "    clf_ada_unweighted.fit(X_train, y_train)\n",
    "\n",
    "    return clf_ada_unweighted\n",
    "\n",
    "\n",
    "def train_weighted_adaboost(df_train):\n",
    "    X_train = df_train[X_COLS]\n",
    "    y_train = df_train['Class']\n",
    "\n",
    "    clf_ada_weighted = AdaBoostClassifier(n_estimators=10)\n",
    "    clf_ada_weighted.fit(X_train, y_train, sample_weight=df_train['C_misclf'])\n",
    "\n",
    "    return clf_ada_weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e5497f",
   "metadata": {},
   "source": [
    "## 6.2 Run cross-validation\n",
    "\n",
    "### 6.2.1 Create folds\n",
    "\n",
    "We will run a 5-fold cross-validation. To create the folds, we can use [`StratifiedKFold` of SciKit-Learn](https://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold). With `StratifiedKFold`, each split is guaranteed to contain a comparable distribution of fraudulent and non-fraudulent transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23cfc4d-d683-4efa-a944-cad6cde6e34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855b7598-6a8f-4991-bcf3-b7a30aebcee2",
   "metadata": {},
   "source": [
    "On a `StratifiedKFold` object, you can call [the method `split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold.split) to generate the splits for a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa25fa6-fdc9-4ac3-a15f-c579da091e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trainval, df_test = get_train_test_dfs()\n",
    "\n",
    "X_trainval = df_trainval[X_COLS]\n",
    "y_trainval = df_trainval['Class']\n",
    "\n",
    "split_idxs = skf.split(X_trainval, y_trainval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdca45d-05f9-477d-ae5b-c943db097d3e",
   "metadata": {},
   "source": [
    "### 6.2.2  Loop over splits\n",
    "\n",
    "In each iteration, we use $4$ folds for training with each of the above training methods, and the other fold for validation. For each validation result, we append a new Python dictionary to the list `results`.\n",
    "\n",
    "Note that `split()` returns the *indices* of the splits, not the samples themselves. In the `for`-loop below, you can indeed see that we use the `train_index` and `val_index` to select rows from `df_trainval`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb302b7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for train_index, val_index in split_idxs:\n",
    "    # Create train and val set from given indices\n",
    "    df_train = df_trainval.iloc[train_index]\n",
    "    df_val = df_trainval.iloc[val_index]\n",
    "\n",
    "    X_val = df_val[X_COLS]\n",
    "    y_val = df_val['Class']\n",
    "    amounts = df_val['Amount']\n",
    "\n",
    "    # 0. Logistic regression on unmodified dataset\n",
    "    print('0. Baseline...')\n",
    "    clf_baseline = train_baseline(df_train)\n",
    "    y_pred_baseline = clf_baseline.predict(X_val)\n",
    "    eval_baseline = evaluate_pred(y_val, y_pred_baseline, amounts)\n",
    "    results.append({\n",
    "        'Method': 'Baseline',\n",
    "        **eval_baseline\n",
    "    })\n",
    "\n",
    "    # 1. Sample weighting\n",
    "    print('1. Sample weighting...')\n",
    "    clf_weighted = train_sample_weighted(df_train)\n",
    "    y_pred_weighted = clf_weighted.predict(X_val)\n",
    "    eval_weighted = evaluate_pred(y_val, y_pred_weighted, amounts)\n",
    "    results.append({\n",
    "        'Method': 'Sample weighting',\n",
    "        **eval_weighted\n",
    "    })\n",
    "\n",
    "    # 2. Subsampling\n",
    "    print('2. Subsampling...')\n",
    "    clf_subsamp = train_subsampled(df_train)\n",
    "    y_pred_subsamp = clf_subsamp.predict(X_val)\n",
    "    eval_subsamp = evaluate_pred(y_val, y_pred_subsamp, amounts)\n",
    "    results.append({\n",
    "        'Method': 'Subsampling',\n",
    "        **eval_subsamp\n",
    "    })\n",
    "\n",
    "    # 3. Cost-sensitive sampling\n",
    "    print('3. Cost-sensitive sampling')\n",
    "    clf_cs_samp = train_cs_sampling(df_train)\n",
    "    y_pred_cs_samp = clf_cs_samp.predict(X_val)\n",
    "    eval_cs_samp = evaluate_pred(y_val, y_pred_cs_samp, amounts)\n",
    "    results.append({\n",
    "        'Method': 'Cost-sensitive sampling',\n",
    "        **eval_cs_samp\n",
    "    })\n",
    "\n",
    "    # 4. Cost-sensitive threhold\n",
    "    print('4. Cost-sensitive threhold')\n",
    "    y_pred_cs_threshold = get_y_pred_cs_threshold(clf_baseline, df_val)\n",
    "    eval_cs_threshold = evaluate_pred(y_val, y_pred_cs_threshold, amounts)\n",
    "    results.append({\n",
    "        'Method': 'Cost-sensitive threhold',\n",
    "        **eval_cs_threshold\n",
    "    })\n",
    "\n",
    "    # 5a. AdaBoost, unweighted\n",
    "    print('5a. AdaBoost, unweighted...')\n",
    "    clf_unwght_ada = train_unweighted_adaboost(df_train)\n",
    "    y_pred_unwght_ada = clf_unwght_ada.predict(X_val)\n",
    "    eval_unwght_ada = evaluate_pred(y_val, y_pred_unwght_ada, amounts)\n",
    "    results.append({\n",
    "        'Method': 'AdaBoost (unweighted)',\n",
    "        **eval_unwght_ada\n",
    "    })\n",
    "\n",
    "    # 5b. AdaBoost, weighted\n",
    "    print('5b. AdaBoost, weighted...')\n",
    "    clf_wght_ada = train_weighted_adaboost(df_train)\n",
    "    y_pred_wght_ada = clf_wght_ada.predict(X_val)\n",
    "    eval_wght_ada = evaluate_pred(y_val, y_pred_wght_ada, amounts)\n",
    "    results.append({\n",
    "        'Method': 'AdaBoost (weighted)',\n",
    "        **eval_wght_ada\n",
    "    })\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2c4219-35c8-4df0-8c2e-c6bdbdd35c28",
   "metadata": {},
   "source": [
    "## 6.3 Inspect the results\n",
    "\n",
    "We start by converting the list of dictionaries `results` into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd3939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539b5961-3052-4376-ac24-a6f7dec9dc3b",
   "metadata": {},
   "source": [
    "To obtain a single metric, we can compute the *F1-score*. This is the harmonic mean between the precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c90acec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results['Cost F1'] = (\n",
    "    2 * df_results['Cost Precision'] * df_results['Cost Recall']\n",
    "    / (df_results['Cost Precision'] + df_results['Cost Recall'])\n",
    ")\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9ff9c8-754e-4589-a271-ba27d7c7e5a5",
   "metadata": {},
   "source": [
    "To obtain a single F1-score per training method, we can [group by](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) the `'Method'` column and compute the mean per group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ff4576-734c-4bda-a014-ddf41fd5fa08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_agg = df_results.groupby('Method').mean()\n",
    "df_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b288343-e411-45fa-879c-62a42b55cac6",
   "metadata": {},
   "source": [
    "Finally, we can sort the aggregated DataFrame by the `'Cost F1'` column to easily see the methods that work best on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31a2151",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg.sort_values(by='Cost F1', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3902e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
